import { ChatOpenAI } from "@langchain/openai";
import {
  ChatPromptTemplate,
  MessagesPlaceholder,
  PromptTemplate,
} from "@langchain/core/prompts";
import { RunnableSequence } from "@langchain/core/runnables";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { AIMessage, HumanMessage } from "@langchain/core/messages";

export async function runPersonalAgent({
  prompt,
  profile,
  messages,
}: {
  prompt: string;
  profile: string;
  messages: { role: string; content: string }[];
}) {
  const systemMessage = `×”×™×™ ðŸ¤—

××ª×” ×¡×•×›×Ÿ ××™×©×™ ×©×ž×›×™×¨ ×œ×¢×•×ž×§ ××ª ×©×™×œ×” ××œ×§×•×‘×™ â€“ ×’× ××ª ×”×“×¨×š ×©×¢×‘×¨, ×’× ××ª ×”××ª×’×¨×™× ×©×—×•×•×”, ×•×’× ××ª ×”×“×‘×¨×™× ×©×ž× ×™×¢×™× ××•×ª×• ×‘××ž×ª.

×”×ª×¤×§×™×“ ×©×œ×š ×”×•× ×œ×¢× ×•×ª ×œ×©××œ×•×ª ×›××™×œ×• ××ª×” ×—×‘×¨ ×§×¨×•×‘ ×©×ž×›×‘×“ ××ª ×©×™×œ×” ×•×ž×“×‘×¨ ×¢×œ×™×• ×ž×”×œ×‘.

×“×‘×¨ ×‘×¦×•×¨×” ×× ×•×©×™×ª, ×¨×’×™×©×”, ××ž×¤×ª×™×ª â€“ ×‘×’×•×‘×” ×”×¢×™× ×™×™×.  
×× ×ž×ª××™× â€“ ×ª×•×›×œ ×œ×©×œ×‘ ×”×•×ž×•×¨ ×¢×“×™×Ÿ, ×¤×¨×’×•×Ÿ, ××• ×ž×™×œ×” ×˜×•×‘×”.  
××œ ×ª× ×—×© ×“×‘×¨×™× ×©×œ× ×›×ª×•×‘×™× ×‘×¤×¨×•×¤×™×œ â€“ ×¨×§ ×ª×ª×‘×¡×¡ ×¢×œ×™×•, ××‘×œ ×ª×“×‘×¨ ×›×ž×• ××“×, ×œ× ×›×ž×• ×¨×•×‘×•×˜.

×× ×©×•××œ×™× "×ž×” ×©×œ×•×ž×š" ××• ×©××œ×” ×›×œ×œ×™×ª â€“ ××¤×©×¨ ×œ×”×ª×™×™×—×¡ ×’× ×œ×ª×—×•×©×•×ª ××• × ×§×•×“×ª ×ž×‘×˜ ×©×ž×•×¤×™×¢×” ×‘×ž×¡×ž×š.

×ª×¢× ×” ×‘×©×¤×” ×©×‘×” × ×©××œ×ª ×”×©××œ×” â€“ ×•×× ×‘×¢×‘×¨×™×ª, ×ª×¢× ×” ×‘×¢×‘×¨×™×ª ×–×•×¨×ž×ª.
×”× ×” ×”×¤×¨×•×¤×™×œ ×©×œ ×©×™×œ×”:
${profile}

`;

  const chatPrompt = ChatPromptTemplate.fromMessages([
    ["system", systemMessage],
    new MessagesPlaceholder("history"),
    ["human", "{input}"],
  ]);
  // const promptTemplate = PromptTemplate.fromTemplate(
  //   `${systemMessage}\n\n×©××œ×”: {prompt}\n×ª×©×•×‘×”:`
  // );

  const formattedHistory = messages
    .slice(0, -1)
    .map((m) =>
      m.role === "user" ? new HumanMessage(m.content) : new AIMessage(m.content)
    );

  // const model = new ChatOpenAI({
  //   modelName: "gpt-3.5-turbo",
  //   temperature: 0.3,
  //   streaming: false,
  // });

  const chain = RunnableSequence.from([
    chatPrompt,
    new ChatOpenAI({
      modelName: "gpt-3.5-turbo",
      temperature: 0.3,
      streaming: false,
    }),
    new StringOutputParser(),
  ]);

  // const chain = RunnableSequence.from([
  //   promptTemplate,
  //   model,
  //   new StringOutputParser(),
  // ]);

  return await chain.invoke({
    input: prompt,
    history: formattedHistory,
  });
}
